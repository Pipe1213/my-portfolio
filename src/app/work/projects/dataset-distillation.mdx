---
title: "Self-Supervised Dataset Distillation"
publishedAt: "2024-05-20"
tags: ["DL", "Distillation", "Self-Supervised"]
links:
  - { label: "GitHub", href: "https://github.com/Pipe1213/LT_Project" }
summary: "Exploration of dataset distillation ideas for transfer learning. Small synthetic sets, competitive accuracy."
images:
  - "/images/distill-cover.jpg"
link: "https://github.com/Pipe1213/LT_Project"
---

**What I tried**
- Create compact synthetic datasets that still transfer well.
- Compare to training from scratch on full data vs. distilled subsets.

**Results**
- Promising accuracy on small models; strong speed-ups for quick experiments.
